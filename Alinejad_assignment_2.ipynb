{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "generative_ai_disabled": true,
      "name": "Alinejad_assignment-2.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaraAlinejad/vae_test_1/blob/main/Alinejad_assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Sara Alinejad***"
      ],
      "metadata": {
        "id": "DInsYIgQZbNq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chemical Applications of Machine Learning (CHEM 4930/5610) - Spring 2026\n",
        "\n",
        "### Assignment 2 - Deadline 2/3/2026\n",
        "Points 10\n",
        "\n",
        "#### General Comments\n",
        "All figures and graph should have approriate labels on the two axis, and should include a legend with appropriate labels of the different plots.\n",
        "\n",
        "The notebook should be return in working format. That is, I should be able to reset all the output and re-run all the cells and get the same results as you obtained."
      ],
      "metadata": {
        "id": "mCl_XQfPCKtb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You should start by saving a copy of the notebook to your Google Drive so you preserve all changes.**\n",
        "\n",
        "**Please add your name as a suffix to the filname**"
      ],
      "metadata": {
        "id": "DJv55sxojLl-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2xGg9pFN9reK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Student Name**: Add your name here\n",
        "\n",
        "**AI usage statement:**\n",
        "Here you should give a statement about any usage of AI tools to assist you with the coding."
      ],
      "metadata": {
        "id": "Putv2cBPVon5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1 - 10 points\n",
        "\n",
        "In this task, we will consider the Bradley Melting Point Dataset, which is curated chemical dataset with melting points of around 3,000 chemical compounds, see [here](https://www.kaggle.com/datasets/aliffaagnur/melting-point-chemical-dataset/data).\n",
        "\n",
        "This dataset is stored in a comma-separated values (csv) file, which is common format used to start data in text files. We can load this into a pandas DataFrame using the `load_csv` function.\n",
        "\n",
        "In this dataset, we have the compounds names, SMILES strings, and the melting point in Celsius.\n",
        "\n",
        "#### A)\n",
        "Identify in the dataset the chemical compounds with the 5 lowest melting points and 5 highest melting points and visualize their 2D chemical structure using RDKit and the [mols2grid package](https://mols2grid.readthedocs.io/en/latest/), where you display the melting point values on the grid, see [here](https://colab.research.google.com/github/PatWalters/practical_cheminformatics_tutorials/blob/main/fundamentals/A_Whirlwind_Introduction_To_The_RDKit.ipynb#scrollTo=N3CR7rMF3sg7) for an example of the usage of mols2grid.\n",
        "\n",
        "#### B)\n",
        "Calculate the following properties for the molecules using RDKIt:\n",
        "- The molecular weight\n",
        "- The number of heavy atoms\n",
        "- Number of hydrogen bond acceptors\n",
        "- Number of hydrogen bond donors\n",
        "- [Octanol-water partition coefficient - LogP](https://pubs-acs-org.libproxy.library.unt.edu/doi/10.1021/ci990307l)\n",
        "- [Topological polar surface area (TPSA) descriptor](https://pubs-acs-org.libproxy.library.unt.edu/doi/abs/10.1021/jm000942e)\n",
        "- Topological polar surface area (TPSA) descriptor, including S and P atoms, see [here](https://www.rdkit.org/docs/RDKit_Book.html#implementation-of-the-tpsa-descriptor)\n",
        "\n",
        "Note: for some of the molecules, the TPSA descriptor will give a value of zero. When doing any analysis for the TPSA descriptor, you should ignore these values.\n",
        "\n",
        "#### C)\n",
        "Write out to a new csv file values of all the properties calculated in B) along with the compound names, SMILES strings, and the melting point in Celsius. Here, when writing this file, you should ignore any compounds where the SMILES conversion did not work correctly.\n",
        "\n",
        "#### D)\n",
        "Perform a linear regression analysis using scikit-learn where you look at the correlation of each of the properties calculated in B) with melting temperature. Here, each property should be considered individually.\n",
        "\n",
        "To avoid outliers, filter out (i.e., remove) the compounds with the lowest 10% and the highest 10% melting temperature. Make a histogram that shows this filtering. Furthermore, for each property, filter out the compounds with lowest 10% and highest 10% values (again making a histogram that shows this filtering). Only consider the joint remaining compounds in your linear regression analysis for each property.\n",
        "\n",
        "When performing the linear regression, employ a 70%/30% training/test split.\n",
        "\n",
        "Calculate the coefficient of determination, $R^2$, for both the training dataset and the test dataset and report both.\n",
        "\n",
        "You should make figure that shows the data along with the linear curve coming from the linear regression. In the figure, it should be clear which data points are in the training and test set (e.g., by having them in different colors). Include the $R^2$ values on the figure.\n",
        "\n",
        "From your analysis, which of the properties correlates best with the melting temperature?\n",
        "\n",
        "#### E)\n",
        "For two of the properties from D) (e.g., the ones that correlate best with the melting point), perform [RANSAC](https://en.wikipedia.org/wiki/Random_sample_consensus) regression, which is method that takes outliers into account when performing linear regression and does not include them in the final modeling, see [here](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ransac.html).\n",
        "\n",
        "In the figure, it should be clear which data points are in inlier set and which are in the outlier set (e.g., by showing them in different colors).\n"
      ],
      "metadata": {
        "id": "21OlZAEAEQXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AI usage statement: in some cases I used AI to solve errors, improving the comments both in term of dictation and also clearity. also for some questions, I was curiouse how chatgpt whould write a code and I have added them here as \"second approch; commented\"**"
      ],
      "metadata": {
        "id": "SrcanfKCRmDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bash script to download all the dataset. Don't worry if you don't understand it\n",
        "%%bash\n",
        "\n",
        "url=\"https://raw.githubusercontent.com/valsson-group/UNT-ChemicalApplicationsOfMachineLearning-Spring2026/refs/heads/main/Assignment-2/\"\n",
        "dataset_filename=\"BradleyDoublePlusGoodMeltingPointDataset.csv\"\n",
        "\n",
        "rm -f ${dataset_filename}\n",
        "\n",
        "wget ${url}/${dataset_filename} &> /dev/null\n",
        "\n",
        "ls"
      ],
      "metadata": {
        "id": "tHzH30mufwos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install rdkit mols2grid\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, RANSACRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "import mols2grid\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors, Lipinski, Crippen, rdMolDescriptors\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from matplotlib.patches import Ellipse\n",
        "from sklearn.neighbors import KernelDensity\n"
      ],
      "metadata": {
        "id": "A5lDlfKjFvsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CSV_PATH = \"BradleyDoublePlusGoodMeltingPointDataset.csv\"\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Columns:\", list(df.columns))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "NXO9cUf2GKb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "#  Data Cleaning and Selection (First aproach; Faster) & Visualization)\n",
        "# ---------------------------------------------------------\n",
        "subset = pd.concat([df.nsmallest(5, 'mpC'), df.nlargest(5, 'mpC')])\n",
        "\n",
        "# Visualize\n",
        "mols2grid.display(subset,\n",
        "                  smiles_col=\"smiles\",\n",
        "                  subset=[\"name\", \"mpC\", \"img\"],\n",
        "                  transform={\"mpC\": lambda x: f\"{x:.1f} °C\"})\n",
        "\n"
      ],
      "metadata": {
        "id": "NUhO3OoKUGdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "#  Data Cleaning and Selection (Second aproach)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Ensuring the melting point column 'mpC' is numeric.\n",
        "# Errors='coerce' turns non-numeric values (like text) into NaN, which we then drop.\n",
        "# df['mpC'] = pd.to_numeric(df['mpC'], errors='coerce')\n",
        "# df_clean = df.dropna(subset=['mpC', 'smiles'])\n",
        "\n",
        "# # Sort the dataset by melting point\n",
        "# df_sorted = df_clean.sort_values(by='mpC')\n",
        "\n",
        "# # Select the 5 lowest and 5 highest melting points\n",
        "# lowest_5 = df_sorted.head(5)\n",
        "# highest_5 = df_sorted.tail(5)\n",
        "\n",
        "# # Combine them into one DataFrame for visualization\n",
        "# results_df = pd.concat([lowest_5, highest_5])\n",
        "\n",
        "# # Let's add a clean label for the grid so we can tell which is which\n",
        "# # We create a new column 'Category'\n",
        "# results_df['Category'] = ['Lowest MP'] * 5 + ['Highest MP'] * 5\n",
        "\n",
        "# print(\"\\nSelected compounds for visualization:\")\n",
        "# print(results_df[['name', 'mpC', 'Category']])\n",
        "\n",
        "\n",
        "# We use the display function to create an interactive grid.\n",
        "# subset: controls what text fields appear on the card (we want name and melting point).\n",
        "# tooltip: controls what appears when you hover over the image.\n",
        "# transform: formats the numbers nicely so we don't get 10 decimal places.\n",
        "\n",
        "# grid = mols2grid.display(\n",
        "#     results_df,\n",
        "#     smiles_col=\"smiles\",   # Tell it where the SMILES strings are\n",
        "#     subset=[\"img\", \"name\", \"mpC\", \"Category\"], # What to show on the card\n",
        "#     tooltip=[\"name\", \"mpC\", \"smiles\"],         # What to show on hover\n",
        "#     transform={\"mpC\": lambda x: f\"{x:.2f} °C\"}, # Format MP nicely\n",
        "#     n_cols=5, # 5 columns to neatly show lowest on row 1 and highest on row 2 (roughly)\n",
        "#     size=(150, 150) # Image size\n",
        "# )\n",
        "\n",
        "# grid"
      ],
      "metadata": {
        "id": "8TMyP3kHQVKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"mol\"] = df[\"smiles\"].apply(Chem.MolFromSmiles)\n",
        "df = df.dropna(subset=[\"mol\"]).copy()\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Calculate molecular properties\n",
        "# ---------------------------------------------------------\n",
        "df[\"MolWt\"]          = df[\"mol\"].apply(Descriptors.MolWt)\n",
        "df[\"HeavyAtomCount\"] = df[\"mol\"].apply(Lipinski.HeavyAtomCount)\n",
        "df[\"HBA\"]            = df[\"mol\"].apply(Lipinski.NumHAcceptors)\n",
        "df[\"HBD\"]            = df[\"mol\"].apply(Lipinski.NumHDonors)\n",
        "df[\"LogP\"]           = df[\"mol\"].apply(Crippen.MolLogP)\n",
        "\n",
        "# TPSA\n",
        "df[\"TPSA\"]     = df[\"mol\"].apply(lambda m: rdMolDescriptors.CalcTPSA(m))\n",
        "df[\"TPSA_S_P\"] = df[\"mol\"].apply(lambda m: rdMolDescriptors.CalcTPSA(m, includeSandP=True))\n",
        "\n",
        "df[[\"name\", \"mpC\", \"MolWt\", \"HeavyAtomCount\", \"HBA\", \"HBD\", \"LogP\", \"TPSA\", \"TPSA_S_P\"]].head()"
      ],
      "metadata": {
        "id": "zhi-ho8XOI4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_filename = \"bradley_melting_point_properties.csv\"\n",
        "\n",
        "columns_to_save = [\n",
        "    \"name\", \"smiles\", \"mpC\",\n",
        "    \"MolWt\", \"HeavyAtomCount\", \"HBA\", \"HBD\",\n",
        "    \"LogP\", \"TPSA\", \"TPSA_S_P\"\n",
        "]\n",
        "\n",
        "df[columns_to_save].to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"Saved {len(df)} compounds to '{output_filename}'.\")"
      ],
      "metadata": {
        "id": "zq3540UmQjRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_verify = pd.read_csv(\"bradley_melting_point_properties.csv\")\n",
        "print(df_verify.columns)\n"
      ],
      "metadata": {
        "id": "kuRDbdBQRgXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mp_low, mp_high = df_verify[\"mpC\"].quantile([0.1, 0.9])\n",
        "\n",
        "plt.hist(df_verify[\"mpC\"], bins=40, alpha=0.6, label=\"All data\")\n",
        "plt.axvline(mp_low, color=\"red\", linestyle=\"--\", label=\"10% cutoff\")\n",
        "plt.axvline(mp_high, color=\"red\", linestyle=\"--\", label=\"90% cutoff\")\n",
        "plt.xlabel(\"Melting Point (°C)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.legend()\n",
        "plt.title(\"Melting Point Filtering\")\n",
        "plt.show()\n",
        "\n",
        "df_mp = df_verify[(df_verify[\"mpC\"] >= mp_low) & (df_verify[\"mpC\"] <= mp_high)].copy()\n"
      ],
      "metadata": {
        "id": "XsU0VRGIT0wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we had it in task B\n",
        "properties = [\n",
        "    \"MolWt\",\n",
        "    \"HeavyAtomCount\",\n",
        "    \"HBA\",\n",
        "    \"HBD\",\n",
        "    \"LogP\",\n",
        "    \"TPSA\",\n",
        "    \"TPSA_S_P\"\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "for prop in properties:\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # Filter property outliers (10%–90%)\n",
        "    # ---------------------------------------------------------\n",
        "    low, high = df_mp[prop].quantile([0.1, 0.9])\n",
        "\n",
        "    plt.hist(df_mp[prop], bins=40, alpha=0.7)\n",
        "    plt.axvline(low, color=\"red\", linestyle=\"--\")\n",
        "    plt.axvline(high, color=\"red\", linestyle=\"--\")\n",
        "    plt.xlabel(prop)\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.title(f\"{prop} Filtering\")\n",
        "    plt.show()\n",
        "\n",
        "    df_filt = df_mp[(df_mp[prop] >= low) & (df_mp[prop] <= high)].copy()\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # Prepare data\n",
        "    # ---------------------------------------------------------\n",
        "    X = df_filt[[prop]].values\n",
        "    y = df_filt[\"mpC\"].values\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=42\n",
        "    )\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # Linear regression\n",
        "    # ---------------------------------------------------------\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred  = model.predict(X_test)\n",
        "    r2_train = r2_score(y_train, y_train_pred)\n",
        "    r2_test  = r2_score(y_test, y_test_pred)\n",
        "    results.append((prop, r2_train, r2_test))\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # Plot data + regression line\n",
        "    # ---------------------------------------------------------\n",
        "    x_line = np.linspace(X.min(), X.max(), 200).reshape(-1, 1)\n",
        "    y_line = model.predict(x_line)\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.scatter(X_train, y_train, color=\"blue\", alpha=0.6, label=\"Train\")\n",
        "    plt.scatter(X_test, y_test, color=\"orange\", alpha=0.6, label=\"Test\")\n",
        "    plt.plot(x_line, y_line, color=\"black\", linewidth=2, label=\"Linear fit\")\n",
        "    plt.xlabel(prop)\n",
        "    plt.ylabel(\"Melting Point (°C)\")\n",
        "    plt.title(\n",
        "        f\"{prop} vs Melting Point\\n\"\n",
        "        f\"$R^2_{{train}}$ = {r2_train:.3f}, \"\n",
        "        f\"$R^2_{{test}}$ = {r2_test:.3f}\"\n",
        "    )\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Uoq8hVrdTdku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame(\n",
        "    results, columns=[\"Property\", \"R2_train\", \"R2_test\"]\n",
        ")\n",
        "\n",
        "results_df.sort_values(\"R2_test\", ascending=False)\n"
      ],
      "metadata": {
        "id": "is2aBN4lVUKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mp_low, mp_high = df[\"mpC\"].quantile([0.1, 0.9])\n",
        "df_mp = df[(df[\"mpC\"] >= mp_low) & (df[\"mpC\"] <= mp_high)].copy()\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Choose two best-correlated properties\n",
        "# ---------------------------------------------------------\n",
        "top_props = [\"TPSA\", \"HBD\"]\n",
        "\n",
        "print(\"Using properties:\", top_props)"
      ],
      "metadata": {
        "id": "iyXX7gC0XLFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2 - Optional 5 points\n",
        "\n",
        "Here we will consider a dataset of two variables $x$ and $y$ sampled from a two-dimensional probability density $P(x,y)$ that is unknown.\n",
        "\n",
        "The dataset is given as a time series in the file `Dataset_RotatedWQ-Potential.data`.\n",
        "\n",
        "The main task is to perform a Gaussian Mixture Model analysis on this two-dimensional dataset.\n",
        "\n",
        "#### A)\n",
        "Plot the dataset, both the time series and also a scatter plot for the $x$ and $y$ variables.\n",
        "\n",
        "Looking at the scatter plot, how many Gaussian components do you think are needed in the Gaussian Mixture Model analysis?\n",
        "\n",
        "#### B)\n",
        "Using Seaborn (or scikit-learn) estimate the two-dimensional probability density $P(x,y)$ using kernel density estimation.\n",
        "\n",
        "#### C)\n",
        "Perform a Gaussian Mixture Model analysis for a different number of components, and obtain the Bayesian information criterion (bic) and Akaike information criterion (aic) values and based on them identify the optimal number of components (remember that for both a lower value is better).\n",
        "\n",
        "#### D)\n",
        "For the optimal number of components, perform a final Gaussian Mixture Model analysis that you will analyze.\n",
        "\n",
        "- What is the weight of each Gaussian components.\n",
        "\n",
        "- What is the percentage of samples that are hard classifed to each cluster.\n",
        "\n",
        "- Make a scatter plot that shows how the samples are hard classifed to each cluster. In this plot, indicate the center of each Gaussian components.\n",
        "\n",
        "- Make figures that shows how the samples are soft classifed to each cluster (e.g., the probablity that they belong to a given cluster). In each plot, indicate the center of corresponding Gaussian components.\n",
        "\n",
        "- Plot a two-dimensional surface of the $P(x,y)$ estimated by the Gaussian Mixture Model. How does this compare to the KDE plot from B)?\n"
      ],
      "metadata": {
        "id": "PJ7zS06xPVJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bash script to download all the dataset. Don't worry if you don't understand it\n",
        "%%bash\n",
        "\n",
        "url=\"https://raw.githubusercontent.com/valsson-group/UNT-ChemicalApplicationsOfMachineLearning-Spring2026/refs/heads/main/Assignment-2/\"\n",
        "dataset_filename=\"Dataset_RotatedWQ-Potential.data\"\n",
        "\n",
        "rm -f ${dataset_filename}\n",
        "\n",
        "wget ${url}/${dataset_filename} &> /dev/null\n",
        "\n",
        "ls\n",
        "\n"
      ],
      "metadata": {
        "id": "PLEPeyqIVcRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'Dataset_RotatedWQ-Potential.data'\n",
        "\n",
        "# 1. Useing r'\\s+' (raw string) to fix the SyntaxWarning\n",
        "# 2. Useing comment='#' to skip any header comments in the file\n",
        "df = pd.read_csv(filename, sep=r'\\s+', header=None, names=['x', 'y'], comment='#')\n",
        "\n",
        "# Forceing columns to be numeric (this fixes the TypeError if \"x\" or \"y\" contained text)\n",
        "df['x'] = pd.to_numeric(df['x'], errors='coerce')\n",
        "df['y'] = pd.to_numeric(df['y'], errors='coerce')\n",
        "\n",
        "# Droping any rows that failed to convert (e.g., text headers inside the file)\n",
        "df = df.dropna()\n",
        "\n",
        "print(f\"Cleaned data: {len(df)} valid time steps.\")"
      ],
      "metadata": {
        "id": "iPzmnVQkcfwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot 1: Time Series\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(df.index, df['x'], label='x variable', alpha=0.7, linewidth=0.5)\n",
        "plt.plot(df.index, df['y'], label='y variable', alpha=0.7, linewidth=0.5, color='orange')\n",
        "plt.title('Time Series Trace')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Coordinate Value')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "NWPjEGGub3_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot 2: Phase Space (Scatter)\n",
        "plt.figure(figsize=(6, 6))\n",
        "# Using hexbin is often clearer for large density datasets than scatter\n",
        "plt.hexbin(df['x'], df['y'], gridsize=50, cmap='inferno', mincnt=1)\n",
        "plt.title('2D Scatter Plot (Density)')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.axis('equal')\n",
        "plt.colorbar(label='Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cArcruq4cuxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[['x','y']].std())"
      ],
      "metadata": {
        "id": "w93UHVpAianK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {'bandwidth': [0.05, 0.06, 0.07, 0.08, 0.09, 0.10]}\n",
        "grid = GridSearchCV(KernelDensity(kernel='gaussian'), params)\n",
        "grid.fit(df[['x','y']].values)\n",
        "\n",
        "print(\"Optimal bandwidth:\", grid.best_params_['bandwidth'])\n"
      ],
      "metadata": {
        "id": "LB_wSNWLjYkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# Method 1: Seaborn\n",
        "# ---------------------------------------------------------\n",
        "# This automatically selects the bandwidth (using Scott's rule) and handles the grid.\n",
        "# thresh=0 clipping ensures we don't plot extremely low probability regions.\n",
        "\n",
        "plt.figure(figsize=(7, 6))\n",
        "sns.kdeplot(\n",
        "    data=df, x='x', y='y',\n",
        "    fill=True,           # Fill the contours\n",
        "    thresh=0.01,         # Lowest iso-contour level to draw\n",
        "    levels=15,           # Number of contour levels (more = smoother)\n",
        "    cmap=\"viridis\",      # Standard scientific colormap\n",
        "    cbar=True            # Add a colorbar\n",
        ")\n",
        "plt.title('2D Kernel Density Estimation (Seaborn)')\n",
        "plt.xlabel('x (Reaction Coordinate 1)')\n",
        "plt.ylabel('y (Reaction Coordinate 2)')\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Method 2: Scikit-learn (Quantitative & Precise)\n",
        "# ---------------------------------------------------------\n",
        "# 1. This approach gives us the actual P(x,y) values on a grid\n",
        "\n",
        "# A value of 0.1  is a reasonable starting point based on the test above.\n",
        "kde = KernelDensity(bandwidth=0.1, kernel='gaussian')\n",
        "\n",
        "data_values = df[['x', 'y']].values\n",
        "kde.fit(data_values)\n",
        "\n",
        "#  Creating a grid to evaluate the density\n",
        "x_grid = np.linspace(df['x'].min() - 0.5, df['x'].max() + 0.5, 100)\n",
        "y_grid = np.linspace(df['y'].min() - 0.5, df['y'].max() + 0.5, 100)\n",
        "X, Y = np.meshgrid(x_grid, y_grid)\n",
        "xy_sample = np.vstack([X.ravel(), Y.ravel()]).T\n",
        "\n",
        "# returns log-density by default\n",
        "log_density = kde.score_samples(xy_sample)\n",
        "density = np.exp(log_density).reshape(X.shape)\n",
        "plt.figure(figsize=(7, 6))\n",
        "\n",
        "# Using contourf to visualize the scalar field P(x,y)\n",
        "contour = plt.contourf(X, Y, density, levels=20, cmap='inferno')\n",
        "plt.colorbar(contour, label='Probability Density $P(x,y)$')\n",
        "plt.title('2D Kernel Density Estimation (Scikit-learn)')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "151ajef2fqBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We use n_components=2 based on visual inspection of the 2D plot\n",
        "gmm = GaussianMixture(n_components=2, covariance_type='full', random_state=42)\n",
        "gmm.fit(df[['x', 'y']])\n",
        "\n",
        "# Extracting parameters\n",
        "print(\"GMM Analysis Results:\")\n",
        "print(\"-\" * 30)\n",
        "for i in range(gmm.n_components):\n",
        "    weight = gmm.weights_[i]\n",
        "    mean = gmm.means_[i]\n",
        "    print(f\"Component {i+1}:\")\n",
        "    print(f\"  Weight: {weight:.3f}\")\n",
        "    print(f\"  Mean:   ({mean[0]:.3f}, {mean[1]:.3f})\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "# Visualize the Gaussians on top of the data\n",
        "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
        "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
        "    ax = ax or plt.gca()\n",
        "\n",
        "    # Convert covariance to principal axes\n",
        "    if covariance.shape == (2, 2):\n",
        "        U, s, Vt = np.linalg.svd(covariance)\n",
        "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
        "        width, height = 2 * np.sqrt(s)\n",
        "    else:\n",
        "        angle = 0\n",
        "        width, height = 2 * np.sqrt(covariance)\n",
        "\n",
        "    # Draw the Ellipse\n",
        "    for nsig in [1, 2, 3]: # Draw contours at 1, 2, and 3 sigma\n",
        "        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n",
        "                             angle=angle, **kwargs, fill=False))\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(df['x'], df['y'], s=1, alpha=0.3, color='gray', label='Data')\n",
        "\n",
        "# Plot the GMM centers and ellipses\n",
        "colors = ['red', 'blue']\n",
        "for i in range(gmm.n_components):\n",
        "    # Plot center\n",
        "    plt.scatter(gmm.means_[i, 0], gmm.means_[i, 1], c=colors[i], s=100, marker='X', zorder=10)\n",
        "    # Plot ellipses\n",
        "    draw_ellipse(gmm.means_[i], gmm.covariances_[i], edgecolor=colors[i], linestyle='--')\n",
        "\n",
        "plt.title('Gaussian Mixture Model Fit (2 Components)')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c27TK_QbbUJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.loadtxt(\"Dataset_RotatedWQ-Potential.data\")\n",
        "\n",
        "X = data[:, :2]\n",
        "\n",
        "n_components_range = range(1, 7)\n",
        "\n",
        "aic_values = []\n",
        "bic_values = []\n",
        "\n",
        "for n in n_components_range:\n",
        "    gmm = GaussianMixture(\n",
        "        n_components=n,\n",
        "        #we can define n_initial but I did'nt see any effect of that on the result\n",
        "        covariance_type=\"full\",\n",
        "        random_state=42\n",
        "    )\n",
        "    gmm.fit(X)\n",
        "\n",
        "    aic_values.append(gmm.aic(X))\n",
        "    bic_values.append(gmm.bic(X))\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(n_components_range, aic_values, marker=\"o\", label=\"AIC\")\n",
        "plt.plot(n_components_range, bic_values, marker=\"o\", label=\"BIC\")\n",
        "\n",
        "plt.xlabel(\"Number of Gaussian Components\")\n",
        "plt.ylabel(\"Information Criterion\")\n",
        "plt.title(\"GMM Model Selection using AIC and BIC\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Nmh4uhGsmbVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_aic_n = n_components_range[np.argmin(aic_values)]\n",
        "best_bic_n = n_components_range[np.argmin(bic_values)]\n",
        "\n",
        "print(\"Optimal number of components based on AIC:\", best_aic_n)\n",
        "print(\"Optimal number of components based on BIC:\", best_bic_n)\n"
      ],
      "metadata": {
        "id": "7WoNUBbomw9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Based on previous finding, we set n_components = 4\n",
        "optimal_n = 4\n",
        "gmm = GaussianMixture(n_components=optimal_n, covariance_type='full', random_state=42)\n",
        "gmm.fit(df[['x', 'y']])\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Weights and Hard Classification Stats\n",
        "# ---------------------------------------------------------\n",
        "weights = gmm.weights_\n",
        "hard_labels = gmm.predict(df[['x', 'y']])\n",
        "\n",
        "# Calculating percentage of samples in each cluster\n",
        "# We use a Series to count efficiently\n",
        "counts = pd.Series(hard_labels).value_counts(normalize=True).sort_index()\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(f\"{'Component':<12} | {'Weight (Model)':<15} | {'Hard Class % (Data)':<20}\")\n",
        "print(\"-\" * 50)\n",
        "for i in range(optimal_n):\n",
        "    # Weight: The mixing coefficient in the mathematical model\n",
        "    # Hard Class %: The actual fraction of points assigned to this cluster\n",
        "    print(f\"Cluster {i+1:<4} | {weights[i]:.4f}          | {counts.get(i, 0):.4%}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Hard Classification Scatter Plot\n",
        "# ---------------------------------------------------------\n",
        "plt.figure(figsize=(8, 6))\n",
        "# Scatter points colored by their hard label\n",
        "scatter = plt.scatter(df['x'], df['y'], c=hard_labels, cmap='tab10', s=5, alpha=0.5)\n",
        "# Plot Centers\n",
        "plt.scatter(gmm.means_[:, 0], gmm.means_[:, 1], c='black', s=200, marker='X', label='Centers', edgecolors='white')\n",
        "plt.title(f'Hard Classification (n={optimal_n})')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.colorbar(scatter, label='Cluster Label')\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Soft Classification Plots (Probabilities)\n",
        "# ---------------------------------------------------------\n",
        "# We get the probability matrix: (n_samples, n_components)\n",
        "soft_probs = gmm.predict_proba(df[['x', 'y']])\n",
        "\n",
        "# Create a grid of subplots to show probability for EACH cluster\n",
        "rows = (optimal_n + 2) // 3  # Calculate required rows for 3 columns\n",
        "fig, axes = plt.subplots(rows, 3, figsize=(15, 4 * rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in range(optimal_n):\n",
        "    ax = axes[i]\n",
        "    # Color points by probability of belonging to Cluster i\n",
        "    # We use a sequential colormap (Reds) so white = 0%, Dark Red = 100%\n",
        "    sc = ax.scatter(df['x'], df['y'], c=soft_probs[:, i], cmap='Reds', s=2, alpha=0.3, vmin=0, vmax=1)\n",
        "\n",
        "    # Mark the center of ONLY this component\n",
        "    ax.scatter(gmm.means_[i, 0], gmm.means_[i, 1], c='blue', s=150, marker='X')\n",
        "\n",
        "    ax.set_title(f'Prob. of belonging to Cluster {i+1}')\n",
        "    ax.axis('equal')\n",
        "    fig.colorbar(sc, ax=ax, label='Probability')\n",
        "\n",
        "# Hide empty subplots (there was 2 of them)\n",
        "for j in range(i + 1, len(axes)):\n",
        "    axes[j].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# GMM Density Surface (P(x,y))\n",
        "# ---------------------------------------------------------\n",
        "x_grid = np.linspace(df['x'].min() - 0.5, df['x'].max() + 0.5, 100)\n",
        "y_grid = np.linspace(df['y'].min() - 0.5, df['y'].max() + 0.5, 100)\n",
        "XX, YY = np.meshgrid(x_grid, y_grid)\n",
        "XY = np.vstack([XX.ravel(), YY.ravel()]).T\n",
        "\n",
        "\n",
        "# score_samples returns log-probability, so we take exp()\n",
        "Z = np.exp(gmm.score_samples(XY))\n",
        "Z = Z.reshape(XX.shape)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "contour = plt.contourf(XX, YY, Z, levels=20, cmap='inferno')\n",
        "plt.colorbar(contour, label='GMM Density P(x,y)')\n",
        "plt.title(f'GMM Estimated Density Surface (n={optimal_n})')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z4VEu1xFme4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Gaussian Mixture Model with n = 4 components was fitted to the data. The component weights reflect how the samples are distributed among the main metastable regions. Hard classification assigns each point to the most likely cluster, and the resulting cluster populations are consistent with these weights.\n",
        "Soft classification shows smooth transitions between clusters, indicating overlap between the Gaussian components. The GMM estimate of P(x,y) captures the main basins seen in the KDE plot, but is smoother and more structured. While KDE resolves finer details, the GMM provides a simpler and more interpretable description of the dominant states."
      ],
      "metadata": {
        "id": "0wX-lAetq5OU"
      }
    }
  ]
}